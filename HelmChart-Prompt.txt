utlining the results from my various testing of encrypted and best practice storage architecture for our GLerp implementations.  Our storage is broken down into three different philosophies/approaches to implementing comprehensively.
MariaDB
Database files will go to longhorn-crypto-mariadb-rwo storageClass.  This provides Longhorn encryption at rest using RWO access for pods with preference to mount the directly connected volume.
Site info (apps.txt, common_site_config.json, etc)  
Use longhorn-crypto-rwm storageClass.  This must be RWM (Read Write Many) files shared by all pods. 
Tested successfully using Longhorn LUKS with GLerp deployment on production cluster
Requires Longhorn version 1.10.1-hotfix-2.  I had to specify this image for the manager in helm chart.  Upgraded both dev and production RKE2 longhorn instances.
We can keep the volume relatively "small" at 5 Gig per site and then use Minio to provide customers with "large" storage needs.
Object Storage (pictures, documents, logos that are uploaded or attached by clients into the system)  
Requires install of custom application dpf_external_storage.  
https://github.com/developmentforpeople/dfp_external_storage
This is an active community supported S3 storage custom application for Frappe/ERPNext version 16.  It provides a means to set up attachments to any/multiple minio storage deployments and select what folders/files will use local (longhorn luks) vs S3 (Minio).  It provides minio proxy via frappe and therefore provides the RBAC controls necessary to limit user accesses by role/permissions.
I had to be inventive on testing this and use my devcontainer pointed at a minio instance on the production RKE2 cluster.
At this point in time, I believe the best way forward for this is to first have the object storage use Longhorn LUKS upon installation of the site.  This combines our site data and object storage as we see them within the gunicorn pod today with an unencrypted longhorn.  While this works, it isn't considered best practice as minio is more resilient and efficient.  So, first install, then use the custom application to manually provide the configuration for minio (accesskey/secretkey) and internal service url.  The custom application is pretty neat in how it allows the migration of files with clicks in the UI, which we probably should limit to system users and admin users. 

NOTE:
Backing up each site on a nightly or weekly basis will require additional evaluation.  The normal bench backup tools won't thoroughly work to capture database, object storage, site info like it does today.  We will need to consider backing up using longhorn and minio mechanisms instead.  We will need to consider "how" and "where" we store customer information remotely.  Good news is, we won't need to be overly concerned with data leaks, because customer data will already be encrypted.




################


The following is me trying to provide the multiple yaml files for implementing a minio-tenant, necessary Kubernetes RBAC/SA, and capturing the vault commands.  We can talk about this on a call, but I'm hoping you two can review and consider how, or if even we should, incorporate this into our helm chart.  The last part is what I feel would be the working helm chart (hopefully, but I cannot fully test until we get the custom application into the image).  I manually did the yaml files, but I believe those can be incorporated into the helm chart templates with the values being streamlined to "enable" installing a minio-instance (pre-requisite is the vault commands).  



PER ERPNEXT/GLERP DEPLOYMENT AND AUTOMATION NOTES. (REPEATABLE)
*Install a minio-tenant, a MariaDB DB per GLerp (installed from helm chart), a GLerp bench per client, use Longhorn LUKs.  Variable List:
#Client name VARIABLE - {{helmVar_TENANT_NAMESPACE}}
#Domain name Variable - {{helmVar_TENANT_NAMESPACE}}


#Create a namespace if not exists
apiVersion: v1
kind: Namespace
metadata:
  name: '{{helmVar_TENANT_NAMESPACE}}'
---
#Single ServiceAccount for glerp and supporting apps: minio-tenant, mariadb, etc for namespace as whole
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: glerp
  name: {{helmVar_TENANT_NAMESPACE}}-sa
  namespace: {{helmVar_TENANT_NAMESPACE}}
---
#RBAC so serviceAccount can read k8 secrets
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: secret-reader
  namespace: {{helmVar_TENANT_NAMESPACE}}
rules:
  - apiGroups: [""]
    resources: ["secrets"]
    verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: secret-reader-binding
  namespace: {{helmVar_TENANT_NAMESPACE}}
subjects:
  - kind: ServiceAccount
    name: {{helmVar_TENANT_NAMESPACE}}-sa
    namespace: {{helmVar_TENANT_NAMESPACE}}
roleRef:
  kind: Role
  name: secret-reader
  apiGroup: rbac.authorization.k8s.io
---
#copy trust certificate using ESO into namespace:
apiVersion: external-secrets.io/v1
kind: ExternalSecret
metadata:
  finalizers:
    - externalsecrets.external-secrets.io/externalsecret-cleanup
  name: 'apps-ca-trust'
  namespace: {{helmVar_TENANT_NAMESPACE}}
spec:
  data:
    - remoteRef:
        conversionStrategy: Default
        decodingStrategy: None
        key: root-ca-secret
        metadataPolicy: None
        property: tls.crt
      secretKey: root_ca
    - remoteRef:
        conversionStrategy: Default
        decodingStrategy: None
        key: apps-int-ca-secret
        metadataPolicy: None
        property: tls.crt
      secretKey: intermediate_ca
  refreshInterval: 1h0m0s
  secretStoreRef:
    kind: ClusterSecretStore
    name: kubernetes-token-auth
  target:
    creationPolicy: Owner
    deletionPolicy: Retain
    name: apps-ca-trust
    template:
      data:
        ca.crt: |
          {{ .root_ca }}
          {{ .intermediate_ca }}
        intermediate.crt: |
          {{ .intermediate_ca }}
        root.crt: |
          {{ .root_ca }}
      engineVersion: v2
      mergePolicy: Replace
      metadata: {}
      type: Opaque
---
#Create a GLerp leaf cert to use within application pods of namespace
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
#  labels:
#    app.kubernetes.io/name: glerp
  name: '{{helmVar_TENANT_NAMESPACE}}-leaf-cert'
  namespace: {{helmVar_TENANT_NAMESPACE}}
spec:
  dnsNames:
    - "*.{{helmVar_TENANT_NAMESPACE}}.svc"
    - "*.{{helmVar_TENANT_NAMESPACE}}.svc.cluster.local"
    - "*.{{helmVar_DOMAIN}}"
  duration: 2160h
  ipAddresses:
    - 127.0.0.1
    - '::1'
  issuerRef:
    group: cert-manager.io
    kind: ClusterIssuer
    name: apps-general-signer
  renewBefore: 360h
  secretName: {{helmVar_TENANT_NAMESPACE}}-leaf-cert
---
#Use ESO to create the pre-existing secret for MINIO tenant install; will be "unavailable" until secret created in vault
apiVersion: external-secrets.io/v1beta1 # Ensure you use beta1 or v1
kind: ExternalSecret
metadata:
  name: {{helmVar_TENANT_NAMESPACE}}-minio-creds
  namespace: {{helmVar_TENANT_NAMESPACE}}
spec:
  refreshInterval: 1h
  secretStoreRef:
    kind: ClusterSecretStore
    name: vault-backend
  target:
    name: {{helmVar_TENANT_NAMESPACE}}-minio-creds # This is the K8s Secret name
    creationPolicy: Owner
    template:
      engineVersion: v2
      data:
        # 1. Standalone keys for GLerp to access mino tenant (accessed via secretKeyRef)
        accesskey: "{{ .accesskey }}"
        secretkey: "{{ .secretkey }}"
       
        # 2. Consolidated file for MinIO Tenant bootstrap (accessed via EnvFrom)
        config.env: |
          export MINIO_ROOT_USER="{{ .accesskey }}"
          export MINIO_ROOT_PASSWORD="{{ .secretkey }}"
  data:
    - secretKey: accesskey
      remoteRef:
        key: secret/data/greenllama/minio-creds
        property: accesskey
    - secretKey: secretkey
      remoteRef:
        key: secret/data/greenllama/minio-creds
        property: secretkey
---
#Add Ingress route to traefik for user browsers to upload/download documents to minio storage.  Will need to get working, and then see about moving to mTLS via Istio.
apiVersion: traefik.io/v1alpha1
kind: IngressRoute
metadata:
  name: minio-{{helmVar_TENANT_NAMESPACE}}
  namespace: {{helmVar_TENANT_NAMESPACE}}
  annotations:
    kubernetes.io/ingress.class: traefik
spec:
  entryPoints:
    - websecure
  tls:
    secretName: letsencrypt-greenllama-tech-tls
  routes:
    - match: Host(`minio-{{helmVar_TENANT_NAMESPACE}}.{{helmVar_DOMAIN}}`) && PathPrefix(`/`)
      kind: Rule
      services:
        - name: minio   #"minio" is hard coded default in minio tenant crd; namespace seperates instances
          port: 80


#VAULT RBAC FOR APPS IN NAMESPACE; commands to run in vault pod shell
# Create the Policy for {{helmVar_TENANT_NAMESPACE}}, MinIO, and any other {{helmVar_TENANT_NAMESPACE}} app Secrets
cd /tmp
cat <<EOF > {{helmVar_TENANT_NAMESPACE}}-policy.hcl
path "secret/data/{{helmVar_TENANT_NAMESPACE}}/minio-creds" {
  capabilities = ["read"]
}
path "secret/data/{{helmVar_TENANT_NAMESPACE}}/app-creds" {
  capabilities = ["read"]
}
# Allow reading the dynamically generated credentials from the CNPG role
path "database/creds/{{helmVar_TENANT_NAMESPACE}}-app-role" {
  capabilities = ["read"]
}
# Allow listing for convenience
path "secret/metadata/{{helmVar_TENANT_NAMESPACE}}/*" {
  capabilities = ["list"]
}
path "auth/kubernetes/role/{{helmVar_TENANT_NAMESPACE}}-app-role" {
  capabilities = ["read"]
}
EOF
vault policy write {{helmVar_TENANT_NAMESPACE}}-policy {{helmVar_TENANT_NAMESPACE}}-policy.hcl

#Create the Vault Role for glerp application (anything under {{helmVar_TENANT_NAMESPACE}} namespace) access; minio uses secret generated by eso-universal-reader role
vault write auth/kubernetes/role/{{helmVar_TENANT_NAMESPACE}}-app-role \
  bound_service_account_names={{helmVar_TENANT_NAMESPACE}}-sa \
  bound_service_account_namespaces={{helmVar_TENANT_NAMESPACE}} \
  policies={{helmVar_TENANT_NAMESPACE}}-policy \
  audience="https://kubernetes.default.svc.cluster.local" \
  ttl=24h



###MINIO###


#will need to run openssl commands in some linux platform to generate random, then cut paste in command.  Need to figure out way of automating with helm chart CI/CD setting; else this will remain manual and outside of helm chart.
vault kv put secret/{{helmVar_TENANT_NAMESPACE}}/minio-creds \\
  accesskey="$(openssl rand -base64 12)" \\
  secretkey="$(openssl rand -base64 24)"


#the minio-tenant crd
apiVersion: minio.min.io/v2
kind: Tenant
metadata:
  name: {{helmVar_TENANT_NAMESPACE}}-minio-tenant
  namespace: {{helmVar_TENANT_NAMESPACE}}
spec:
  buckets:
    - name: {{helmVar_TENANT_NAMESPACE}}
      objectLock: false # Standard setting, set to true if required
      region: us-east-1 # Use the region MinIO is configured for, or a default
  mountPath: /export
  configuration:
    name: {{helmVar_TENANT_NAMESPACE}}-minio-creds
  env:
    - name: MINIO_SERVER_URL
      value: https://minio-{{helmVar_TENANT_NAMESPACE}}.{{helmVar_DOMAIN}}
    - name: MINIO_BROWSER_REDIRECT_URL
      value: https://minio-{{helmVar_TENANT_NAMESPACE}}.{{helmVar_DOMAIN}}
    - name: MINIO_KMS_VAULT_ENABLE
      value: "on"
    - name: MINIO_KMS_VAULT_ENDPOINT
      value: "http://vault-active.vault-system.svc.cluster.local:8200"
    - name: MINIO_KMS_VAULT_AUTH_TYPE
      value: "jwt"
    - name: MINIO_KMS_VAULT_ROLE
      value: "{{helmVar_TENANT_NAMESPACE}}-role"       # the Vault role mapped to your MinIO service account
    - name: MINIO_KMS_VAULT_KEY_NAME
      value: "minio-key"            # your Vault Transit key name
    - name: MINIO_KMS_VAULT_JWT_PATH
      value: "/var/run/secrets/kubernetes.io/serviceaccount/token"
  pools:
    - name: pool-0
      servers: 2
      volumesPerServer: 2
      volumeClaimTemplate:
        metadata:
          name: data
        spec:
          accessModes:
            - ReadWriteOnce
          storageClassName: directpv-min-io
          resources:
            requests:
              storage: 512Mi
      containerSecurityContext:
        runAsUser: 1000
        runAsGroup: 1000
        runAsNonRoot: true
        allowPrivilegeEscalation: false
      securityContext:
        fsGroup: 1000
    - name: pool-1
      servers: 2
      volumesPerServer: 2
      volumeClaimTemplate:
        metadata:
          name: data
        spec:
          accessModes:
            - ReadWriteOnce
          storageClassName: directpv-min-io
          resources:
            requests:
              storage: 512Mi
      containerSecurityContext:
        runAsUser: 1000
        runAsGroup: 1000
        runAsNonRoot: true
        allowPrivilegeEscalation: false
      securityContext:
        fsGroup: 1000
  #for istio sidecar injection.  Specify inboundPort for mTLS mesh for only UI/service to service connections, do not include minio dataplane oriented traffic.
  poolsMetadata:
    annotations:
      sidecar.istio.io/inject: 'true'
      traffic.sidecar.istio.io/includeInboundPorts: '10443'
      traffic.sidecar.istio.io/includeOutboundPorts: '8200'
      traffic.sidecar.istio.io/excludeInboundPorts: '9000, 9090'
      traffic.sidecar.istio.io/excludeOutboundPorts: '9000, 9090'

  requestAutoCert: false
  prometheusOperator: false
  features:
    bucketDNS: false
    enableSFTP: false
  podManagementPolicy: Parallel
  subPath: /data
  serviceAccountName: {{helmVar_TENANT_NAMESPACE}}-sa






####
glerp helm chart version 1.0.12
####
dragonfly-cache:
  enabled: true
dragonfly-queue:
  enabled: true
  storage:
    enabled: false
    size: 8Gi
externalRedis:
  cache: ''
  queue: ''
extraObjects:
  - apiVersion: traefik.io/v1alpha1
    kind: IngressRoute
    metadata:
      annotations:
        kubernetes.io/ingress.class: traefik
      name: {{helmVar_TENANT_NAMESPACE}}
      namespace: traefik-system
    spec:
      entryPoints:
        - websecure
      routes:
        - kind: Rule
          match: Host(`{{helmVar_TENANT_NAMESPACE}}.{{helmVar_DOMAIN}}`) && PathPrefix(`/`)
          services:
            - name: glerp
              namespace: {{helmVar_TENANT_NAMESPACE}}
              port: 8080
              scheme: http
      tls:
        secretName: letsencrypt-greenllama-tech-tls
fullnameOverride: ''
httproute:
  annotations: {}
  enabled: false
  hostnames:
    - erp.example.com
  name: ''
  parentRefs:
    - gatewayName: public-gateway
      gatewayNamespace: networking
      gatewaySectionName: http
  rules:
    - matches:
        - path: /
          pathType: PathPrefix
image:
  pullPolicy: Always
  repository: ghcr.io/green-llama/glerp-image
  tag: dev
imagePullSecrets:
  - name: ghcr-cred
ingress:
  annotations: {}
  enabled: false
  hosts:
    - host: erp.cluster.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []
jobs:
  backup:
    affinity: {}
    backoffLimit: 0
    enabled: false
    nodeSelector: {}
    resources: {}
    siteName: {{helmVar_TENANT_NAMESPACE}}.{{helmVar_DOMAIN}}
    tolerations: []
    withFiles: true
  configure:
    affinity: {}
    args: []
    backoffLimit: 0
    command: []
    enabled: true
    envVars: []
    fixVolume: true
    nodeSelector: {}
    resources: {}
    tolerations: []
  createSite:
    adminExistingSecret: ''
    adminExistingSecretKey: password
    adminPassword: changeit
    affinity: {}
    backoffLimit: 0
    dbType: mariadb
    enabled: true
    forceCreate: false
    installApps:
      - erpnext
      - frappe_theme
      - erpnext_email_parser
      - hrms
      - glerp_audience
      - newsletter
      - blog
      - eps
      - offsite_backups
      - posawesome
      - payments
      - webshop
      - glerp_braintree_webhooks
      - dfp_external_storage
      - glerp_frappe_theme
      - glerp_branding

    nodeSelector: {}
    resources: {}
    siteName: {{helmVar_TENANT_NAMESPACE}}.{{helmVar_DOMAIN}}
    tolerations: []
  custom:
    affinity: {}
    backoffLimit: 0
    containers: []
    enabled: false
    initContainers: []
    jobName: ''
    labels: {}
    nodeSelector: {}
    restartPolicy: Never
    tolerations: []
    volumes: []
  dropSite:
    affinity: {}
    backoffLimit: 0
    enabled: false
    forced: false
    nodeSelector: {}
    resources: {}
    siteName: erp.cluster.local
    tolerations: []
  migrate:
    affinity: {}
    backoffLimit: 0
    enabled: false
    nodeSelector: {}
    resources: {}
    siteName: erp.cluster.local
    skipFailing: false
    tolerations: []
  volumePermissions:
    affinity: {}
    backoffLimit: 0
    enabled: false
    nodeSelector: {}
    resources: {}
    tolerations: []
mariadb:
  enabled: false
mariadb-sts:
  enabled: true
  image:
    pullPolicy: IfNotPresent
    repository: mariadb
    tag: '10.6'
  myCnf: |
    [mysqld]
    skip-character-set-client-handshake
    skip-innodb-read-only-compressed
    character-set-server=utf8mb4
    collation-server=utf8mb4_unicode_ci
  persistence:
    size: 8Gi
    storageClass: longhorn-crypto-mariadb-rwo
  resources: {}
  rootPassword: changeit     #we need to consider installing, then using vault db engine to take over managing this via ESO and K8 secrets.
mariadb-subchart:
  enabled: false
  image:
    repository: bitnamilegacy/mariadb
    tag: 10.6.17-debian-11-r10
nameOverride: ''
nginx:
  affinity: {}
  autoscaling:
    enabled: false
    maxReplicas: 3
    minReplicas: 1
    targetCPU: 75
    targetMemory: 75
  defaultTopologySpread:
    maxSkew: 1
    topologyKey: kubernetes.io/hostname
    whenUnsatisfiable: DoNotSchedule
  envVars: []
  environment:
    clientMaxBodySize: 50m
    frappeSiteNameHeader: $host
    proxyReadTimeout: '120'
    upstreamRealIPAddress: 127.0.0.1
    upstreamRealIPHeader: X-Forwarded-For
    upstreamRealIPRecursive: 'off'
  initContainers: []
  livenessProbe:
    initialDelaySeconds: 5
    periodSeconds: 10
    tcpSocket:
      port: 8080
  nodeSelector: {}
  readinessProbe:
    initialDelaySeconds: 5
    periodSeconds: 10
    tcpSocket:
      port: 8080
  replicaCount: 1
  resources: {}
  service:
    port: 8080
    type: ClusterIP
  sidecars: []
  tolerations: []
persistence:
  logs:
    accessModes:
      - ReadWriteMany
    enabled: false
    size: 5Gi
  worker:
    accessModes:
      - ReadWriteMany
    enabled: true
    size: 5Gi
    storageClass: longhorn-crypto-rwm
podSecurityContext:
  supplementalGroups:
    - 1000
postgresql:
  enabled: false
postgresql-sts:
  enabled: false
  image:
    pullPolicy: IfNotPresent
    repository: postgres
    tag: '15'
  persistence:
    size: 8Gi
  postgresPassword: changeit
  postgresUser: postgres
  resources: {}
postgresql-subchart:
  enabled: false
  image:
    repository: bitnamilegacy/postgresql
    tag: '14'
redis-cache:
  enabled: false
  image:
    repository: bitnamilegacy/redis
    tag: '7.0'
redis-queue:
  enabled: false
  image:
    repository: bitnamilegacy/redis
    tag: '7.0'
securityContext:
  capabilities:
    add:
      - CAP_CHOWN
serviceAccount:
  create: true
socketio:
  affinity: {}
  autoscaling:
    enabled: false
    maxReplicas: 3
    minReplicas: 1
    targetCPU: 75
    targetMemory: 75
  envVars: []
  initContainers: []
  livenessProbe:
    initialDelaySeconds: 5
    periodSeconds: 10
    tcpSocket:
      port: 9000
  nodeSelector: {}
  readinessProbe:
    initialDelaySeconds: 5
    periodSeconds: 10
    tcpSocket:
      port: 9000
  replicaCount: 1
  resources: {}
  service:
    port: 9000
    type: ClusterIP
  sidecars: []
  tolerations: []
worker:
  default:
    affinity: {}
    autoscaling:
      enabled: false
      maxReplicas: 3
      minReplicas: 1
      targetCPU: 75
      targetMemory: 75
    envVars: []
    initContainers: []
    livenessProbe:
      override: false
      probe: {}
    nodeSelector: {}
    readinessProbe:
      override: false
      probe: {}
    replicaCount: 1
    resources: {}
    sidecars: []
    tolerations: []
  defaultTopologySpread:
    maxSkew: 1
    topologyKey: kubernetes.io/hostname
    whenUnsatisfiable: DoNotSchedule
  gunicorn:
    affinity: {}
    args: []
    autoscaling:
      enabled: false
      maxReplicas: 3
      minReplicas: 1
      targetCPU: 75
      targetMemory: 75
    envVars: []
    initContainers: []
    livenessProbe:
      initialDelaySeconds: 5
      periodSeconds: 10
      tcpSocket:
        port: 8000
    nodeSelector: {}
    readinessProbe:
      initialDelaySeconds: 5
      periodSeconds: 10
      tcpSocket:
        port: 8000
    replicaCount: 1
    resources: {}
    service:
      port: 8000
      type: ClusterIP
    sidecars: []
    tolerations: []
  healthProbe: |
    exec:
      command:
        - bash
        - -c
        - |-
          echo "Pinging backing services";
          {{- if (index .Values "mariadb-sts").enabled }}
          wait-for-it {{ include "erpnext.fullname" . }}-mariadb-sts:3306 -t 1;
          {{- else if (index .Values "postgresql-sts").enabled }}
          wait-for-it {{ include "erpnext.fullname" . }}-postgresql-sts:5432 -t 1;
          {{- else if or .Values.mariadb.enabled (get .Values "mariadb-subchart").enabled }}
          (
            wait-for-it {{ .Release.Name }}-mariadb-subchart:3306 -t 0 || \
            wait-for-it {{ .Release.Name }}-mariadb:3306 -t 0 || \
            wait-for-it {{ .Release.Name }}-mariadb-subchart-primary:3306 -t 0 || \
            wait-for-it {{ .Release.Name }}-mariadb-primary:3306 -t 1
          )
          {{- else if or .Values.postgresql.enabled (get .Values "postgresql-subchart").enabled }}
          (
            wait-for-it {{ .Release.Name }}-postgresql-subchart:5432 -t 0 || \
            wait-for-it {{ .Release.Name }}-postgresql:5432 -t 1
          )
          {{- else if or .Values.postgresql.enabled (index .Values "postgresql-subchart").enabled }}
          wait-for-it {{ .Release.Name }}-postgresql-subchart:5432 -t 1;
          {{- else if .Values.dbHost }}
          wait-for-it {{ .Values.dbHost }}:{{ .Values.dbPort }} -t 1;
          {{- end }}
          {{- if .Values.externalRedis.cache }}
          wait-for-it $(echo {{ .Values.externalRedis.cache }} | sed 's,redis://,,') -t 1;
          {{- else if (index .Values "dragonfly-cache").enabled }}
          wait-for-it {{ .Release.Name }}-dragonfly-cache:6379 -t 1;
          {{- else if (index .Values "redis-cache").enabled }}
          wait-for-it {{ .Release.Name }}-redis-cache-master:6379 -t 1;
          {{- end }}
          {{- if .Values.externalRedis.queue }}
          wait-for-it $(echo {{ .Values.externalRedis.queue }} | sed 's,redis://,,') -t 1;
          {{- else if (index .Values "dragonfly-queue").enabled }}
          wait-for-it {{ .Release.Name }}-dragonfly-queue:6379 -t 1;
          {{- else if (index .Values "redis-queue").enabled }}
          wait-for-it {{ .Release.Name }}-redis-queue-master:6379 -t 1;
          {{- end }}
    initialDelaySeconds: 15
    periodSeconds: 5
    timeoutSeconds: 5
  long:
    affinity: {}
    autoscaling:
      enabled: false
      maxReplicas: 3
      minReplicas: 1
      targetCPU: 75
      targetMemory: 75
    envVars: []
    initContainers: []
    livenessProbe:
      override: false
      probe: {}
    nodeSelector: {}
    readinessProbe:
      override: false
      probe: {}
    replicaCount: 1
    resources: {}
    sidecars: []
    tolerations: []
  scheduler:
    affinity: {}
    envVars: []
    initContainers: []
    livenessProbe:
      override: false
      probe: {}
    nodeSelector: {}
    readinessProbe:
      override: false
      probe: {}
    replicaCount: 1
    resources: {}
    sidecars: []
    tolerations: []
  short:
    affinity: {}
    autoscaling:
      enabled: false
      maxReplicas: 3
      minReplicas: 1
      targetCPU: 75
      targetMemory: 75
    envVars: []
    initContainers: []
    livenessProbe:
      override: false
      probe: {}
    nodeSelector: {}
    readinessProbe:
      override: false
      probe: {}
    replicaCount: 1
    resources: {}
    sidecars: []
    tolerations: []

Under nginx block.  The following environment block works with the capital letter version of the environment variables being sent.  

In this case, I increased from the 50m (Meg) upload max file size to 500m.  We have limits on Traefik as well as within the ERPNext application, so having nginx being a high number should be good in my opinion.  I could only increase and valid test with use of the uppercase variables... the lowercase ones failed to deploy helm chart.

  environment:
    clientMaxBodySize: null
    frappeSiteNameHeader: null
    proxyReadTimeout: null
    upstreamRealIPAddress: null
    upstreamRealIPHeader: null
    upstreamRealIPRecursive: null
    CLIENT_MAX_BODY_SIZE: 500m
    FRAPPE_SITE_NAME_HEADER: $host
    PROXY_READ_TIMEOUT: '120'
    UPSTREAM_REAL_IP_ADDRESS: 127.0.0.1
    UPSTREAM_REAL_IP_HEADER: X-Forwarded-For
    UPSTREAM_REAL_IP_RECURSIVE: 'off'